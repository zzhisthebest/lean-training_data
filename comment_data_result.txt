[{"docString": null,
  "declType":
  "∀ {E : Type u_1} [inst : NormedAddCommGroup E] [inst_1 : InnerProductSpace ℝ E] [inst_2 : CompleteSpace E] {f : E → ℝ}\n  {f' : E → E} {xm x₀ : E} {alg : Gradient_Descent_fix_stepsize f f' x₀},\n  ConvexOn ℝ Set.univ f →\n    Gradient_Descent_fix_stepsize.a f f' x₀ ≤ 1 / ↑(Gradient_Descent_fix_stepsize.l f f' x₀) →\n      ∀ (k : ℕ),\n        f (Gradient_Descent_fix_stepsize.x f f' x₀ (k + 1)) - f xm ≤\n          1 / (2 * (↑k + 1) * Gradient_Descent_fix_stepsize.a f f' x₀) * ‖x₀ - xm‖ ^ 2",
  "declName": "gradient_method",
  "decl":
  "lemma gradient_method (hfun: ConvexOn ℝ Set.univ f) (step₂ : alg.a ≤ 1 / alg.l) :\n    ∀ k : ℕ, f (alg.x (k + 1)) - f xm ≤ 1 / (2 * (k + 1) * alg.a) * ‖x₀ - xm‖ ^ 2 := by\n  intro k\n  have step1₂ : alg.l ≤ 1 / alg.a := by\n    rw [le_one_div alg.step₁] at step₂; exact step₂; linarith [alg.hl]\n  have : LipschitzWith alg.l f' := alg.smooth\n  have : alg.l > 0 := alg.hl\n  have tα : 1 / ((2 : ℝ) * (k + 1) * alg.a) > 0 := by\n    have : alg.a > 0 := alg.step₁\n    positivity\n  obtain xdescent := point_descent_for_convex hfun step₂\n  have mono : ∀ k : ℕ, f (alg.x (k + 1)) ≤ f (alg.x k) := by\n    intro k\n    rw [alg.update k]\n    calc\n      _ ≤ f (alg.x k) - alg.a / 2 *  ‖(f' (alg.x k))‖ ^ 2 :=\n          convex_lipschitz alg.diff this step1₂ alg.step₁ alg.smooth (alg.x k)\n      _ ≤ f (alg.x k) := by\n          simp; apply mul_nonneg (by linarith [alg.step₁]) (sq_nonneg _)\n  have sum_prop : ∀ n : ℕ, (Finset.range (n + 1)).sum (fun (k : ℕ) ↦ f (alg.x (k + 1)) - f xm)\n      ≤ 1 / (2 * alg.a) * (‖x₀ - xm‖ ^ 2 - ‖alg.x (n + 1) - xm‖ ^ 2) := by\n    intro n\n    induction' n with j IH\n    · specialize xdescent (0 : ℕ)\n      simp\n      calc\n        _ ≤ f xm + 1 / (2 * alg.a) * (‖alg.x 0 - xm‖ ^ 2 - ‖alg.x (0 + 1) - xm‖ ^ 2) :=\n            xdescent\n        _ = alg.a⁻¹ * 2⁻¹ * (‖x₀ - xm‖^ 2 - ‖alg.x 1 - xm‖ ^ 2) + f xm := by\n            rw [alg.initial]; simp; ring_nf\n    · specialize xdescent (j + 1)\n      calc\n        _ = (Finset.range (j + 1)).sum (fun (k : ℕ) ↦ f (alg.x (k + 1)) - f xm)\n                + f (alg.x (j + 2)) - f xm := by\n              rw [Finset.sum_range_succ (fun (k : ℕ)↦ f (alg.x (k+1))-f (xm)) j.succ]\n              rw [Nat.succ_eq_add_one j]; ring_nf; rw [add_sub]\n          _ ≤ 1 / (2 * alg.a) * (‖x₀ - xm‖ ^ 2 - ‖alg.x (j + 1) - xm‖ ^ 2)\n              + f (alg.x (j + 2)) - f xm := by linarith\n          _ ≤ 1 / (2 * alg.a) * (‖x₀ - xm‖ ^ 2 - ‖alg.x (j + 1) - xm‖ ^ 2)\n                + 1 / (2 * alg.a) * (‖alg.x (j + 1) - xm‖ ^ 2 - ‖alg.x (j + 2) - xm‖ ^ 2) := by\n              rw [add_sub_right_comm]; linarith\n          _ = 1 / (2 * alg.a) * (‖x₀ - xm‖ ^ 2 - ‖alg.x (j.succ + 1) - xm‖ ^ 2)  := by\n              ring_nf; simp; left; ring_nf\n  obtain sum_prop_1 := mono_sum_prop mono\n  specialize sum_prop_1 k\n  specialize sum_prop k\n  have h : f (alg.x (k + 1)) - f xm ≤ 1 / (2 * (k + 1) * alg.a) *\n     (‖x₀ - xm‖ ^ 2 - ‖alg.x (k + 1) - xm‖ ^ 2) := by\n    have tt1 : 0 ≤ k + (1 : ℝ) := by exact add_nonneg (Nat.cast_nonneg k) zero_le_one\n    calc\n      _ ≤ (Finset.range (k + 1)).sum (fun (k : ℕ) ↦ f (alg.x (k + 1)) - f xm) / (k + 1) :=\n        sum_prop_1\n      _ ≤ 1 / (2 * alg.a) * (‖x₀ - xm‖ ^ 2 - ‖alg.x (k + 1) - xm‖ ^ 2) / (k + 1) :=\n        div_le_div_of_nonneg_right sum_prop tt1\n      _ = 1 / (2 * (k + 1) * alg.a) * (‖x₀ - xm‖ ^ 2 - ‖alg.x (k + 1) - xm‖ ^ 2) := by simp; ring_nf\n  exact le_mul_of_le_mul_of_nonneg_left h (by simp) (by linarith)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\ninstance {f : E → ℝ} {f' : E → E} {x₀ : E} [p : Gradient_Descent_fix_stepsize f f' x₀] :\n    GradientDescent f f' x₀ where\n  x := p.x\n  diff := p.diff\n  a := fun _ ↦ p.a\n  update := p.update\n  l := p.l\n  hl := p.hl\n  smooth := p.smooth\n  step₁ := by simp [p.step₁]\n  initial := p.initial\n\nopen InnerProductSpace Set\n\nvariable {f : E → ℝ} {f' : E → E}\n\nvariable {l : NNReal} {xm x₀ : E}{a : ℝ}\n\nvariable {alg : Gradient_Descent_fix_stepsize f f' x₀}\n\n-- equivalent description of the convexity of a smooth function\nlemma convex_function (h₁ : ∀ x₁ : E, HasGradientAt f (f' x₁) x₁)\n    (hfun: ConvexOn ℝ Set.univ f) :\n  ∀ x y, f x ≤ f y + inner (f' x) (x - y) := by\n  intro x y\n  obtain this := Convex_first_order_condition' (h₁ x) hfun (by trivial) y (by trivial)\n  rw [← neg_sub, inner_neg_right] at this\n  linarith\n\n-- the bound for one step of the gradient method using the Lipschitz continuity of the gradient\nlemma convex_lipschitz (h₁ : ∀ x₁ : E, HasGradientAt f (f' x₁) x₁)\n    (h₂ : l > (0 : ℝ)) (ha₁ : l ≤ 1 / a) (ha₂ : a > 0) (h₃ : LipschitzWith l f') :\n    ∀ x : E, f (x - a • (f' x)) ≤ f x - a / 2 * ‖f' x‖ ^ 2 := by\n  intro x\n  calc\n    _ ≤ f x + inner (f' x) (x - a • (f' x) - x) + l / 2 * ‖x - a • (f' x) - x‖ ^ 2 :=\n        lipschitz_continuos_upper_bound' h₁ h₃ x (x - a • (f' x))\n    _ = f x + ((l.1 / 2 * a * a -a) * ‖f' x‖ ^ 2) := by\n        simp; ring_nf; simp\n        rw [real_inner_smul_right, real_inner_self_eq_norm_sq, norm_smul]; simp\n        rw [abs_of_pos ha₂]; ring_nf\n    _ ≤ f x + (- a / 2*  ‖(f' x)‖ ^2) := by\n        simp only [add_le_add_iff_left, gt_iff_lt, norm_pos_iff, ne_eq]\n        apply mul_le_mul_of_nonneg_right\n        · simp;\n          calc l / 2 * a * a = (l * a) * (a / 2) := by ring_nf\n                _  ≤ 1 * (a / 2) := by\n                  apply mul_le_mul_of_nonneg _ (by linarith) (by positivity) (by linarith)\n                  · exact (le_div_iff₀ ha₂).mp ha₁\n                _  = - a / 2 + a := by ring_nf\n        · exact sq_nonneg ‖f' x‖\n    _ = f x - a / 2 *  ‖f' x‖ ^ 2 := by ring_nf\n\n-- using the point version for the certain iteration of the gradient method\nlemma point_descent_for_convex (hfun : ConvexOn ℝ Set.univ f) (step₂ : alg.a ≤ 1 / alg.l) :\n    ∀ k : ℕ, f (alg.x (k + 1)) ≤ f xm + 1 / ((2 : ℝ) * alg.a)\n      * (‖alg.x k - xm‖ ^ 2 - ‖alg.x (k + 1) - xm‖ ^ 2) := by\n  have step₂ : alg.l ≤ 1 / alg.a := by\n    rw [le_one_div alg.step₁] at step₂; exact step₂; linarith [alg.hl]\n  intro k\n  have : LipschitzWith alg.l f' := alg.smooth\n  have : alg.l > 0 := alg.hl\n  have descent: ∀ x : E, f (x - alg.a • (f' x)) ≤\n      f xm + 1 / ((2 : ℝ) * alg.a) * (‖x - xm‖ ^ 2 - ‖x - alg.a • (f' x) - xm‖ ^ 2) := by\n    intro x\n    have t1 : 1 / ((2 : ℝ) * alg.a) * ((2 : ℝ) * alg.a) = 1 := by\n      field_simp; ring_nf; apply mul_inv_cancel₀; linarith [alg.step₁]\n    have t2 : inner (f' x) (x - xm) - alg.a / 2 * ‖f' x‖ ^ 2 =\n        1 / ((2 : ℝ) * alg.a) * (‖x - xm‖ ^ 2 - ‖x - alg.a • (f' x) - xm‖ ^ 2) := by\n      symm\n      have t2₁ : ‖x - alg.a • (f' x) - xm‖ ^ 2 =\n          ‖x - xm‖ ^ 2 - ((2 : ℝ) * alg.a) * inner  (f' x) (x - xm) + ‖alg.a • (f' x)‖ ^ 2 := by\n        rw [sub_right_comm]; simp [norm_sub_sq_real (x - xm) _]\n        ring_nf; rw [real_inner_smul_right, real_inner_comm];\n      calc\n        _ = 1 / ((2 : ℝ) * alg.a) * ((2 : ℝ) * alg.a) * (inner (f' x) (x - xm))\n              + 1 / ((2 : ℝ) * alg.a) * (- ‖alg.a • (f' x)‖ ^ 2) := by rw [t2₁]; ring_nf\n          _ = inner (f' x) (x - xm) + 1 / ((2 : ℝ) * alg.a)\n              * (- ‖alg.a • (f' x)‖ ^ 2) := by rw [t1, one_mul]\n          _ = inner (f' x) (x - xm) - 1 / ((2 : ℝ) * alg.a) * (alg.a * alg.a) * (‖f' x‖ ^ 2) := by\n              rw [norm_smul _ _]; simp; rw [abs_of_pos alg.step₁]; ring_nf\n          _ = inner (f' x) (x - xm) - alg.a / (2 : ℝ)\n              * ‖f' x‖ ^ 2 := by ring_nf; simp; left; rw [pow_two,mul_self_mul_inv alg.a]\n    calc f (x - alg.a • (f' x)) ≤ f x - alg.a / 2 * ‖f' x‖ ^ 2 := by\n              exact convex_lipschitz alg.diff this step₂ alg.step₁ alg.smooth x\n            _   ≤ f xm + inner (f' x) (x - xm) - alg.a / 2 * ‖f' x‖ ^ 2 := by\n              linarith [convex_function alg.diff hfun x xm]\n            _   = f xm + 1 / ((2 : ℝ) * alg.a) * (‖x - xm‖ ^ 2 - ‖x - alg.a • (f' x) - xm‖ ^ 2) := by\n              rw [add_sub_assoc, t2]\n  specialize descent (alg.x k)\n  rw [alg.update k]\n  exact descent\n\n-- the O(1/t) descent property of the gradient method\n\nlemma gradient_method (hfun: ConvexOn ℝ Set.univ f) (step₂ : alg.a ≤ 1 / alg.l) :\n    ∀ k : ℕ, f (alg.x (k + 1)) - f xm ≤ 1 / (2 * (k + 1) * alg.a) * ‖x₀ - xm‖ ^ 2 := by\n  intro k\n  have step1₂ : alg.l ≤ 1 / alg.a := by\n    rw [le_one_div alg.step₁] at step₂; exact step₂; linarith [alg.hl]\n  have : LipschitzWith alg.l f' := alg.smooth\n  have : alg.l > 0 := alg.hl\n  have tα : 1 / ((2 : ℝ) * (k + 1) * alg.a) > 0 := by\n    have : alg.a > 0 := alg.step₁\n    positivity\n  obtain xdescent := point_descent_for_convex hfun step₂\n  have mono : ∀ k : ℕ, f (alg.x (k + 1)) ≤ f (alg.x k) := by\n    intro k\n    rw [alg.update k]\n    calc\n      _ ≤ f (alg.x k) - alg.a / 2 *  ‖(f' (alg.x k))‖ ^ 2 :=\n          convex_lipschitz alg.diff this step1₂ alg.step₁ alg.smooth (alg.x k)\n      _ ≤ f (alg.x k) := by\n          simp; apply mul_nonneg (by linarith [alg.step₁]) (sq_nonneg _)\n  have sum_prop : ∀ n : ℕ, (Finset.range (n + 1)).sum (fun (k : ℕ) ↦ f (alg.x (k + 1)) - f xm)\n      ≤ 1 / (2 * alg.a) * (‖x₀ - xm‖ ^ 2 - ‖alg.x (n + 1) - xm‖ ^ 2) := by\n    intro n\n    induction' n with j IH\n    · specialize xdescent (0 : ℕ)\n      simp\n      calc\n        _ ≤ f xm + 1 / (2 * alg.a) * (‖alg.x 0 - xm‖ ^ 2 - ‖alg.x (0 + 1) - xm‖ ^ 2) :=\n            xdescent\n        _ = alg.a⁻¹ * 2⁻¹ * (‖x₀ - xm‖^ 2 - ‖alg.x 1 - xm‖ ^ 2) + f xm := by\n            rw [alg.initial]; simp; ring_nf\n    · specialize xdescent (j + 1)\n      calc\n        _ = (Finset.range (j + 1)).sum (fun (k : ℕ) ↦ f (alg.x (k + 1)) - f xm)\n                + f (alg.x (j + 2)) - f xm := by\n              rw [Finset.sum_range_succ (fun (k : ℕ)↦ f (alg.x (k+1))-f (xm)) j.succ]\n              rw [Nat.succ_eq_add_one j]; ring_nf; rw [add_sub]\n          _ ≤ 1 / (2 * alg.a) * (‖x₀ - xm‖ ^ 2 - ‖alg.x (j + 1) - xm‖ ^ 2)\n              + f (alg.x (j + 2)) - f xm := by linarith\n          _ ≤ 1 / (2 * alg.a) * (‖x₀ - xm‖ ^ 2 - ‖alg.x (j + 1) - xm‖ ^ 2)\n                + 1 / (2 * alg.a) * (‖alg.x (j + 1) - xm‖ ^ 2 - ‖alg.x (j + 2) - xm‖ ^ 2) := by\n              rw [add_sub_right_comm]; linarith\n          _ = 1 / (2 * alg.a) * (‖x₀ - xm‖ ^ 2 - ‖alg.x (j.succ + 1) - xm‖ ^ 2)  := by\n              ring_nf; simp; left; ring_nf\n  obtain sum_prop_1 := mono_sum_prop mono\n  specialize sum_prop_1 k\n  specialize sum_prop k\n  have h : f (alg.x (k + 1)) - f xm ≤ 1 / (2 * (k + 1) * alg.a) *\n     (‖x₀ - xm‖ ^ 2 - ‖alg.x (k + 1) - xm‖ ^ 2) := by\n    have tt1 : 0 ≤ k + (1 : ℝ) := by exact add_nonneg (Nat.cast_nonneg k) zero_le_one\n    calc\n      _ ≤ (Finset.range (k + 1)).sum (fun (k : ℕ) ↦ f (alg.x (k + 1)) - f xm) / (k + 1) :=\n        sum_prop_1\n      _ ≤ 1 / (2 * alg.a) * (‖x₀ - xm‖ ^ 2 - ‖alg.x (k + 1) - xm‖ ^ 2) / (k + 1) :=\n        div_le_div_of_nonneg_right sum_prop tt1\n      _ = 1 / (2 * (k + 1) * alg.a) * (‖x₀ - xm‖ ^ 2 - ‖alg.x (k + 1) - xm‖ ^ 2) := by simp; ring_nf\n  exact le_mul_of_le_mul_of_nonneg_left h (by simp) (by linarith)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} [inst : NormedAddCommGroup E] [inst_1 : InnerProductSpace ℝ E] [inst_2 : CompleteSpace E] {f : E → ℝ}\n  {f' : E → E} {xm x₀ : E} {alg : Gradient_Descent_fix_stepsize f f' x₀},\n  ConvexOn ℝ Set.univ f →\n    Gradient_Descent_fix_stepsize.a f f' x₀ ≤ 1 / ↑(Gradient_Descent_fix_stepsize.l f f' x₀) →\n      ∀ (k : ℕ),\n        f (Gradient_Descent_fix_stepsize.x f f' x₀ (k + 1)) ≤\n          f xm +\n            1 / (2 * Gradient_Descent_fix_stepsize.a f f' x₀) *\n              (‖Gradient_Descent_fix_stepsize.x f f' x₀ k - xm‖ ^ 2 -\n                ‖Gradient_Descent_fix_stepsize.x f f' x₀ (k + 1) - xm‖ ^ 2)",
  "declName": "point_descent_for_convex",
  "decl":
  "lemma point_descent_for_convex (hfun : ConvexOn ℝ Set.univ f) (step₂ : alg.a ≤ 1 / alg.l) :\n    ∀ k : ℕ, f (alg.x (k + 1)) ≤ f xm + 1 / ((2 : ℝ) * alg.a)\n      * (‖alg.x k - xm‖ ^ 2 - ‖alg.x (k + 1) - xm‖ ^ 2) := by\n  have step₂ : alg.l ≤ 1 / alg.a := by\n    rw [le_one_div alg.step₁] at step₂; exact step₂; linarith [alg.hl]\n  intro k\n  have : LipschitzWith alg.l f' := alg.smooth\n  have : alg.l > 0 := alg.hl\n  have descent: ∀ x : E, f (x - alg.a • (f' x)) ≤\n      f xm + 1 / ((2 : ℝ) * alg.a) * (‖x - xm‖ ^ 2 - ‖x - alg.a • (f' x) - xm‖ ^ 2) := by\n    intro x\n    have t1 : 1 / ((2 : ℝ) * alg.a) * ((2 : ℝ) * alg.a) = 1 := by\n      field_simp; ring_nf; apply mul_inv_cancel₀; linarith [alg.step₁]\n    have t2 : inner (f' x) (x - xm) - alg.a / 2 * ‖f' x‖ ^ 2 =\n        1 / ((2 : ℝ) * alg.a) * (‖x - xm‖ ^ 2 - ‖x - alg.a • (f' x) - xm‖ ^ 2) := by\n      symm\n      have t2₁ : ‖x - alg.a • (f' x) - xm‖ ^ 2 =\n          ‖x - xm‖ ^ 2 - ((2 : ℝ) * alg.a) * inner  (f' x) (x - xm) + ‖alg.a • (f' x)‖ ^ 2 := by\n        rw [sub_right_comm]; simp [norm_sub_sq_real (x - xm) _]\n        ring_nf; rw [real_inner_smul_right, real_inner_comm];\n      calc\n        _ = 1 / ((2 : ℝ) * alg.a) * ((2 : ℝ) * alg.a) * (inner (f' x) (x - xm))\n              + 1 / ((2 : ℝ) * alg.a) * (- ‖alg.a • (f' x)‖ ^ 2) := by rw [t2₁]; ring_nf\n          _ = inner (f' x) (x - xm) + 1 / ((2 : ℝ) * alg.a)\n              * (- ‖alg.a • (f' x)‖ ^ 2) := by rw [t1, one_mul]\n          _ = inner (f' x) (x - xm) - 1 / ((2 : ℝ) * alg.a) * (alg.a * alg.a) * (‖f' x‖ ^ 2) := by\n              rw [norm_smul _ _]; simp; rw [abs_of_pos alg.step₁]; ring_nf\n          _ = inner (f' x) (x - xm) - alg.a / (2 : ℝ)\n              * ‖f' x‖ ^ 2 := by ring_nf; simp; left; rw [pow_two,mul_self_mul_inv alg.a]\n    calc f (x - alg.a • (f' x)) ≤ f x - alg.a / 2 * ‖f' x‖ ^ 2 := by\n              exact convex_lipschitz alg.diff this step₂ alg.step₁ alg.smooth x\n            _   ≤ f xm + inner (f' x) (x - xm) - alg.a / 2 * ‖f' x‖ ^ 2 := by\n              linarith [convex_function alg.diff hfun x xm]\n            _   = f xm + 1 / ((2 : ℝ) * alg.a) * (‖x - xm‖ ^ 2 - ‖x - alg.a • (f' x) - xm‖ ^ 2) := by\n              rw [add_sub_assoc, t2]\n  specialize descent (alg.x k)\n  rw [alg.update k]\n  exact descent\n\n-- the O(1/t) descent property of the gradient method\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\ninstance {f : E → ℝ} {f' : E → E} {x₀ : E} [p : Gradient_Descent_fix_stepsize f f' x₀] :\n    GradientDescent f f' x₀ where\n  x := p.x\n  diff := p.diff\n  a := fun _ ↦ p.a\n  update := p.update\n  l := p.l\n  hl := p.hl\n  smooth := p.smooth\n  step₁ := by simp [p.step₁]\n  initial := p.initial\n\nopen InnerProductSpace Set\n\nvariable {f : E → ℝ} {f' : E → E}\n\nvariable {l : NNReal} {xm x₀ : E}{a : ℝ}\n\nvariable {alg : Gradient_Descent_fix_stepsize f f' x₀}\n\n-- equivalent description of the convexity of a smooth function\nlemma convex_function (h₁ : ∀ x₁ : E, HasGradientAt f (f' x₁) x₁)\n    (hfun: ConvexOn ℝ Set.univ f) :\n  ∀ x y, f x ≤ f y + inner (f' x) (x - y) := by\n  intro x y\n  obtain this := Convex_first_order_condition' (h₁ x) hfun (by trivial) y (by trivial)\n  rw [← neg_sub, inner_neg_right] at this\n  linarith\n\n-- the bound for one step of the gradient method using the Lipschitz continuity of the gradient\nlemma convex_lipschitz (h₁ : ∀ x₁ : E, HasGradientAt f (f' x₁) x₁)\n    (h₂ : l > (0 : ℝ)) (ha₁ : l ≤ 1 / a) (ha₂ : a > 0) (h₃ : LipschitzWith l f') :\n    ∀ x : E, f (x - a • (f' x)) ≤ f x - a / 2 * ‖f' x‖ ^ 2 := by\n  intro x\n  calc\n    _ ≤ f x + inner (f' x) (x - a • (f' x) - x) + l / 2 * ‖x - a • (f' x) - x‖ ^ 2 :=\n        lipschitz_continuos_upper_bound' h₁ h₃ x (x - a • (f' x))\n    _ = f x + ((l.1 / 2 * a * a -a) * ‖f' x‖ ^ 2) := by\n        simp; ring_nf; simp\n        rw [real_inner_smul_right, real_inner_self_eq_norm_sq, norm_smul]; simp\n        rw [abs_of_pos ha₂]; ring_nf\n    _ ≤ f x + (- a / 2*  ‖(f' x)‖ ^2) := by\n        simp only [add_le_add_iff_left, gt_iff_lt, norm_pos_iff, ne_eq]\n        apply mul_le_mul_of_nonneg_right\n        · simp;\n          calc l / 2 * a * a = (l * a) * (a / 2) := by ring_nf\n                _  ≤ 1 * (a / 2) := by\n                  apply mul_le_mul_of_nonneg _ (by linarith) (by positivity) (by linarith)\n                  · exact (le_div_iff₀ ha₂).mp ha₁\n                _  = - a / 2 + a := by ring_nf\n        · exact sq_nonneg ‖f' x‖\n    _ = f x - a / 2 *  ‖f' x‖ ^ 2 := by ring_nf\n\n-- using the point version for the certain iteration of the gradient method\nlemma point_descent_for_convex (hfun : ConvexOn ℝ Set.univ f) (step₂ : alg.a ≤ 1 / alg.l) :\n    ∀ k : ℕ, f (alg.x (k + 1)) ≤ f xm + 1 / ((2 : ℝ) * alg.a)\n      * (‖alg.x k - xm‖ ^ 2 - ‖alg.x (k + 1) - xm‖ ^ 2) := by\n  have step₂ : alg.l ≤ 1 / alg.a := by\n    rw [le_one_div alg.step₁] at step₂; exact step₂; linarith [alg.hl]\n  intro k\n  have : LipschitzWith alg.l f' := alg.smooth\n  have : alg.l > 0 := alg.hl\n  have descent: ∀ x : E, f (x - alg.a • (f' x)) ≤\n      f xm + 1 / ((2 : ℝ) * alg.a) * (‖x - xm‖ ^ 2 - ‖x - alg.a • (f' x) - xm‖ ^ 2) := by\n    intro x\n    have t1 : 1 / ((2 : ℝ) * alg.a) * ((2 : ℝ) * alg.a) = 1 := by\n      field_simp; ring_nf; apply mul_inv_cancel₀; linarith [alg.step₁]\n    have t2 : inner (f' x) (x - xm) - alg.a / 2 * ‖f' x‖ ^ 2 =\n        1 / ((2 : ℝ) * alg.a) * (‖x - xm‖ ^ 2 - ‖x - alg.a • (f' x) - xm‖ ^ 2) := by\n      symm\n      have t2₁ : ‖x - alg.a • (f' x) - xm‖ ^ 2 =\n          ‖x - xm‖ ^ 2 - ((2 : ℝ) * alg.a) * inner  (f' x) (x - xm) + ‖alg.a • (f' x)‖ ^ 2 := by\n        rw [sub_right_comm]; simp [norm_sub_sq_real (x - xm) _]\n        ring_nf; rw [real_inner_smul_right, real_inner_comm];\n      calc\n        _ = 1 / ((2 : ℝ) * alg.a) * ((2 : ℝ) * alg.a) * (inner (f' x) (x - xm))\n              + 1 / ((2 : ℝ) * alg.a) * (- ‖alg.a • (f' x)‖ ^ 2) := by rw [t2₁]; ring_nf\n          _ = inner (f' x) (x - xm) + 1 / ((2 : ℝ) * alg.a)\n              * (- ‖alg.a • (f' x)‖ ^ 2) := by rw [t1, one_mul]\n          _ = inner (f' x) (x - xm) - 1 / ((2 : ℝ) * alg.a) * (alg.a * alg.a) * (‖f' x‖ ^ 2) := by\n              rw [norm_smul _ _]; simp; rw [abs_of_pos alg.step₁]; ring_nf\n          _ = inner (f' x) (x - xm) - alg.a / (2 : ℝ)\n              * ‖f' x‖ ^ 2 := by ring_nf; simp; left; rw [pow_two,mul_self_mul_inv alg.a]\n    calc f (x - alg.a • (f' x)) ≤ f x - alg.a / 2 * ‖f' x‖ ^ 2 := by\n              exact convex_lipschitz alg.diff this step₂ alg.step₁ alg.smooth x\n            _   ≤ f xm + inner (f' x) (x - xm) - alg.a / 2 * ‖f' x‖ ^ 2 := by\n              linarith [convex_function alg.diff hfun x xm]\n            _   = f xm + 1 / ((2 : ℝ) * alg.a) * (‖x - xm‖ ^ 2 - ‖x - alg.a • (f' x) - xm‖ ^ 2) := by\n              rw [add_sub_assoc, t2]\n  specialize descent (alg.x k)\n  rw [alg.update k]\n  exact descent\n\n-- the O(1/t) descent property of the gradient method\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} [inst : NormedAddCommGroup E] [inst_1 : InnerProductSpace ℝ E] [inst_2 : CompleteSpace E] {f : E → ℝ}\n  {f' : E → E} {l : NNReal} {a : ℝ},\n  (∀ (x₁ : E), HasGradientAt f (f' x₁) x₁) →\n    ↑l > 0 → ↑l ≤ 1 / a → a > 0 → LipschitzWith l f' → ∀ (x : E), f (x - a • f' x) ≤ f x - a / 2 * ‖f' x‖ ^ 2",
  "declName": "convex_lipschitz",
  "decl":
  "lemma convex_lipschitz (h₁ : ∀ x₁ : E, HasGradientAt f (f' x₁) x₁)\n    (h₂ : l > (0 : ℝ)) (ha₁ : l ≤ 1 / a) (ha₂ : a > 0) (h₃ : LipschitzWith l f') :\n    ∀ x : E, f (x - a • (f' x)) ≤ f x - a / 2 * ‖f' x‖ ^ 2 := by\n  intro x\n  calc\n    _ ≤ f x + inner (f' x) (x - a • (f' x) - x) + l / 2 * ‖x - a • (f' x) - x‖ ^ 2 :=\n        lipschitz_continuos_upper_bound' h₁ h₃ x (x - a • (f' x))\n    _ = f x + ((l.1 / 2 * a * a -a) * ‖f' x‖ ^ 2) := by\n        simp; ring_nf; simp\n        rw [real_inner_smul_right, real_inner_self_eq_norm_sq, norm_smul]; simp\n        rw [abs_of_pos ha₂]; ring_nf\n    _ ≤ f x + (- a / 2*  ‖(f' x)‖ ^2) := by\n        simp only [add_le_add_iff_left, gt_iff_lt, norm_pos_iff, ne_eq]\n        apply mul_le_mul_of_nonneg_right\n        · simp;\n          calc l / 2 * a * a = (l * a) * (a / 2) := by ring_nf\n                _  ≤ 1 * (a / 2) := by\n                  apply mul_le_mul_of_nonneg _ (by linarith) (by positivity) (by linarith)\n                  · exact (le_div_iff₀ ha₂).mp ha₁\n                _  = - a / 2 + a := by ring_nf\n        · exact sq_nonneg ‖f' x‖\n    _ = f x - a / 2 *  ‖f' x‖ ^ 2 := by ring_nf\n\n-- using the point version for the certain iteration of the gradient method\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\ninstance {f : E → ℝ} {f' : E → E} {x₀ : E} [p : Gradient_Descent_fix_stepsize f f' x₀] :\n    GradientDescent f f' x₀ where\n  x := p.x\n  diff := p.diff\n  a := fun _ ↦ p.a\n  update := p.update\n  l := p.l\n  hl := p.hl\n  smooth := p.smooth\n  step₁ := by simp [p.step₁]\n  initial := p.initial\n\nopen InnerProductSpace Set\n\nvariable {f : E → ℝ} {f' : E → E}\n\nvariable {l : NNReal} {xm x₀ : E}{a : ℝ}\n\nvariable {alg : Gradient_Descent_fix_stepsize f f' x₀}\n\n-- equivalent description of the convexity of a smooth function\nlemma convex_function (h₁ : ∀ x₁ : E, HasGradientAt f (f' x₁) x₁)\n    (hfun: ConvexOn ℝ Set.univ f) :\n  ∀ x y, f x ≤ f y + inner (f' x) (x - y) := by\n  intro x y\n  obtain this := Convex_first_order_condition' (h₁ x) hfun (by trivial) y (by trivial)\n  rw [← neg_sub, inner_neg_right] at this\n  linarith\n\n-- the bound for one step of the gradient method using the Lipschitz continuity of the gradient\nlemma convex_lipschitz (h₁ : ∀ x₁ : E, HasGradientAt f (f' x₁) x₁)\n    (h₂ : l > (0 : ℝ)) (ha₁ : l ≤ 1 / a) (ha₂ : a > 0) (h₃ : LipschitzWith l f') :\n    ∀ x : E, f (x - a • (f' x)) ≤ f x - a / 2 * ‖f' x‖ ^ 2 := by\n  intro x\n  calc\n    _ ≤ f x + inner (f' x) (x - a • (f' x) - x) + l / 2 * ‖x - a • (f' x) - x‖ ^ 2 :=\n        lipschitz_continuos_upper_bound' h₁ h₃ x (x - a • (f' x))\n    _ = f x + ((l.1 / 2 * a * a -a) * ‖f' x‖ ^ 2) := by\n        simp; ring_nf; simp\n        rw [real_inner_smul_right, real_inner_self_eq_norm_sq, norm_smul]; simp\n        rw [abs_of_pos ha₂]; ring_nf\n    _ ≤ f x + (- a / 2*  ‖(f' x)‖ ^2) := by\n        simp only [add_le_add_iff_left, gt_iff_lt, norm_pos_iff, ne_eq]\n        apply mul_le_mul_of_nonneg_right\n        · simp;\n          calc l / 2 * a * a = (l * a) * (a / 2) := by ring_nf\n                _  ≤ 1 * (a / 2) := by\n                  apply mul_le_mul_of_nonneg _ (by linarith) (by positivity) (by linarith)\n                  · exact (le_div_iff₀ ha₂).mp ha₁\n                _  = - a / 2 + a := by ring_nf\n        · exact sq_nonneg ‖f' x‖\n    _ = f x - a / 2 *  ‖f' x‖ ^ 2 := by ring_nf\n\n-- using the point version for the certain iteration of the gradient method\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} [inst : NormedAddCommGroup E] [inst_1 : InnerProductSpace ℝ E] [inst_2 : CompleteSpace E] {f : E → ℝ}\n  {f' : E → E},\n  (∀ (x₁ : E), HasGradientAt f (f' x₁) x₁) → ConvexOn ℝ Set.univ f → ∀ (x y : E), f x ≤ f y + ⟪f' x, x - y⟫_ℝ",
  "declName": "convex_function",
  "decl":
  "lemma convex_function (h₁ : ∀ x₁ : E, HasGradientAt f (f' x₁) x₁)\n    (hfun: ConvexOn ℝ Set.univ f) :\n  ∀ x y, f x ≤ f y + inner (f' x) (x - y) := by\n  intro x y\n  obtain this := Convex_first_order_condition' (h₁ x) hfun (by trivial) y (by trivial)\n  rw [← neg_sub, inner_neg_right] at this\n  linarith\n\n-- the bound for one step of the gradient method using the Lipschitz continuity of the gradient\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\ninstance {f : E → ℝ} {f' : E → E} {x₀ : E} [p : Gradient_Descent_fix_stepsize f f' x₀] :\n    GradientDescent f f' x₀ where\n  x := p.x\n  diff := p.diff\n  a := fun _ ↦ p.a\n  update := p.update\n  l := p.l\n  hl := p.hl\n  smooth := p.smooth\n  step₁ := by simp [p.step₁]\n  initial := p.initial\n\nopen InnerProductSpace Set\n\nvariable {f : E → ℝ} {f' : E → E}\n\nvariable {l : NNReal} {xm x₀ : E}{a : ℝ}\n\nvariable {alg : Gradient_Descent_fix_stepsize f f' x₀}\n\n-- equivalent description of the convexity of a smooth function\nlemma convex_function (h₁ : ∀ x₁ : E, HasGradientAt f (f' x₁) x₁)\n    (hfun: ConvexOn ℝ Set.univ f) :\n  ∀ x y, f x ≤ f y + inner (f' x) (x - y) := by\n  intro x y\n  obtain this := Convex_first_order_condition' (h₁ x) hfun (by trivial) y (by trivial)\n  rw [← neg_sub, inner_neg_right] at this\n  linarith\n\n-- the bound for one step of the gradient method using the Lipschitz continuity of the gradient\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  [inst : NormedAddCommGroup E] →\n    [inst_1 : InnerProductSpace ℝ E] →\n      [inst_2 : CompleteSpace E] →\n        {f : E → ℝ} → {f' : E → E} → {x₀ : E} → [p : Gradient_Descent_fix_stepsize f f' x₀] → GradientDescent f f' x₀",
  "declName": "instGradientDescentOfGradient_Descent_fix_stepsize",
  "decl":
  "instance {f : E → ℝ} {f' : E → E} {x₀ : E} [p : Gradient_Descent_fix_stepsize f f' x₀] :\n    GradientDescent f f' x₀ where\n  x := p.x\n  diff := p.diff\n  a := fun _ ↦ p.a\n  update := p.update\n  l := p.l\n  hl := p.hl\n  smooth := p.smooth\n  step₁ := by simp [p.step₁]\n  initial := p.initial\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\ninstance {f : E → ℝ} {f' : E → E} {x₀ : E} [p : Gradient_Descent_fix_stepsize f f' x₀] :\n    GradientDescent f f' x₀ where\n  x := p.x\n  diff := p.diff\n  a := fun _ ↦ p.a\n  update := p.update\n  l := p.l\n  hl := p.hl\n  smooth := p.smooth\n  step₁ := by simp [p.step₁]\n  initial := p.initial\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {inst : NormedAddCommGroup E} {inst_1 : InnerProductSpace ℝ E} {inst_2 : CompleteSpace E} {f : E → ℝ}\n  {f' : E → E} {x0 : E} [self : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0],\n  @Gradient_Descent_fix_stepsize.a E inst inst_1 inst_2 f f' x0 self > 0",
  "declName": "Gradient_Descent_fix_stepsize.step₁",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {inst : NormedAddCommGroup E} {inst_1 : InnerProductSpace ℝ E} {inst_2 : CompleteSpace E} {f : E → ℝ}\n  {f' : E → E} {x0 : E} [self : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0],\n  ↑(@Gradient_Descent_fix_stepsize.l E inst inst_1 inst_2 f f' x0 self) > 0",
  "declName": "Gradient_Descent_fix_stepsize.hl",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  [inst : NormedAddCommGroup E] →\n    [inst_1 : InnerProductSpace ℝ E] →\n      [inst_2 : CompleteSpace E] →\n        {f : E → ℝ} →\n          {f' : E → E} →\n            {x0 : E} →\n              {motive : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0 → Sort u} →\n                (t : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0) →\n                  ((x : ℕ → E) →\n                      (a : ℝ) →\n                        (l : NNReal) →\n                          (diff : ∀ (x₁ : E), HasGradientAt f (f' x₁) x₁) →\n                            (smooth : LipschitzWith l f') →\n                              (update : ∀ (k : ℕ), x (k + 1) = x k - a • f' (x k)) →\n                                (hl : ↑l > 0) →\n                                  (step₁ : a > 0) →\n                                    (initial : x 0 = x0) →\n                                      motive\n                                        (@Gradient_Descent_fix_stepsize.mk E inst inst_1 inst_2 f f' x0 x a l diff\n                                          smooth update hl step₁ initial)) →\n                    motive t",
  "declName": "Gradient_Descent_fix_stepsize.recOn",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  {inst : NormedAddCommGroup E} →\n    {inst_1 : InnerProductSpace ℝ E} →\n      {inst_2 : CompleteSpace E} →\n        (f : E → ℝ) →\n          (f' : E → E) → (x0 : E) → [self : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0] → ℕ → E",
  "declName": "Gradient_Descent_fix_stepsize.x",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {inst : NormedAddCommGroup E} {inst_1 : InnerProductSpace ℝ E} {inst_2 : CompleteSpace E} {f : E → ℝ}\n  {f' : E → E} (x0 : E) [self : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0] (x₁ : E),\n  HasGradientAt f (f' x₁) x₁",
  "declName": "Gradient_Descent_fix_stepsize.diff",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  [inst : NormedAddCommGroup E] →\n    [inst_1 : InnerProductSpace ℝ E] →\n      [inst_2 : CompleteSpace E] →\n        {f : E → ℝ} →\n          {f' : E → E} →\n            {x0 : E} →\n              {motive : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0 → Sort u} →\n                ((x : ℕ → E) →\n                    (a : ℝ) →\n                      (l : NNReal) →\n                        (diff : ∀ (x₁ : E), HasGradientAt f (f' x₁) x₁) →\n                          (smooth : LipschitzWith l f') →\n                            (update : ∀ (k : ℕ), x (k + 1) = x k - a • f' (x k)) →\n                              (hl : ↑l > 0) →\n                                (step₁ : a > 0) →\n                                  (initial : x 0 = x0) →\n                                    motive\n                                      (@Gradient_Descent_fix_stepsize.mk E inst inst_1 inst_2 f f' x0 x a l diff smooth\n                                        update hl step₁ initial)) →\n                  (t : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0) → motive t",
  "declName": "Gradient_Descent_fix_stepsize.rec",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {inst : NormedAddCommGroup E} {inst_1 : InnerProductSpace ℝ E} {inst_2 : CompleteSpace E} {f : E → ℝ}\n  {f' : E → E} {x0 : E} [self : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0],\n  LipschitzWith (@Gradient_Descent_fix_stepsize.l E inst inst_1 inst_2 f f' x0 self) f'",
  "declName": "Gradient_Descent_fix_stepsize.smooth",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  {inst : NormedAddCommGroup E} →\n    {inst_1 : InnerProductSpace ℝ E} →\n      {inst_2 : CompleteSpace E} →\n        {f : E → ℝ} →\n          {f' : E → E} →\n            {x0 : E} →\n              {P : Sort u} →\n                {v1 v2 : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0} →\n                  v1 = v2 → @Gradient_Descent_fix_stepsize.noConfusionType E inst inst_1 inst_2 f f' x0 P v1 v2",
  "declName": "Gradient_Descent_fix_stepsize.noConfusion",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {inst : NormedAddCommGroup E} {inst_1 : InnerProductSpace ℝ E} {inst_2 : CompleteSpace E} {f : E → ℝ}\n  {f' : E → E} {x0 : E} [self : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0],\n  @Gradient_Descent_fix_stepsize.x E inst inst_1 inst_2 f f' x0 self 0 = x0",
  "declName": "Gradient_Descent_fix_stepsize.initial",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  [inst : NormedAddCommGroup E] →\n    [inst_1 : InnerProductSpace ℝ E] →\n      [inst_2 : CompleteSpace E] →\n        {f : E → ℝ} →\n          {f' : E → E} →\n            {x0 : E} →\n              {motive : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0 → Sort u} →\n                (t : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0) →\n                  ((x : ℕ → E) →\n                      (a : ℝ) →\n                        (l : NNReal) →\n                          (diff : ∀ (x₁ : E), HasGradientAt f (f' x₁) x₁) →\n                            (smooth : LipschitzWith l f') →\n                              (update : ∀ (k : ℕ), x (k + 1) = x k - a • f' (x k)) →\n                                (hl : ↑l > 0) →\n                                  (step₁ : a > 0) →\n                                    (initial : x 0 = x0) →\n                                      motive\n                                        (@Gradient_Descent_fix_stepsize.mk E inst inst_1 inst_2 f f' x0 x a l diff\n                                          smooth update hl step₁ initial)) →\n                    motive t",
  "declName": "Gradient_Descent_fix_stepsize.casesOn",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  {inst : NormedAddCommGroup E} →\n    {inst_1 : InnerProductSpace ℝ E} →\n      {inst_2 : CompleteSpace E} →\n        (f : E → ℝ) → (f' : E → E) → (x0 : E) → [self : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0] → ℝ",
  "declName": "Gradient_Descent_fix_stepsize.a",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  [inst : NormedAddCommGroup E] →\n    [inst_1 : InnerProductSpace ℝ E] → [inst : CompleteSpace E] → (E → ℝ) → (E → E) → E → Type u_1",
  "declName": "Gradient_Descent_fix_stepsize",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  [inst : NormedAddCommGroup E] →\n    [inst_1 : InnerProductSpace ℝ E] →\n      [inst_2 : CompleteSpace E] →\n        {f : E → ℝ} →\n          {f' : E → E} →\n            {x0 : E} →\n              (x : ℕ → E) →\n                (a : ℝ) →\n                  (l : NNReal) →\n                    (∀ (x₁ : E), HasGradientAt f (f' x₁) x₁) →\n                      LipschitzWith l f' →\n                        (∀ (k : ℕ), x (k + 1) = x k - a • f' (x k)) →\n                          ↑l > 0 → a > 0 → x 0 = x0 → @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0",
  "declName": "Gradient_Descent_fix_stepsize.mk",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  {inst : NormedAddCommGroup E} →\n    {inst_1 : InnerProductSpace ℝ E} →\n      {inst_2 : CompleteSpace E} →\n        (f : E → ℝ) →\n          (f' : E → E) → (x0 : E) → [self : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0] → NNReal",
  "declName": "Gradient_Descent_fix_stepsize.l",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {inst : NormedAddCommGroup E} {inst_1 : InnerProductSpace ℝ E} {inst_2 : CompleteSpace E} {f : E → ℝ}\n  {f' : E → E} {x0 : E} [self : @Gradient_Descent_fix_stepsize E inst inst_1 inst_2 f f' x0] (k : ℕ),\n  @Gradient_Descent_fix_stepsize.x E inst inst_1 inst_2 f f' x0 self (k + 1) =\n    @Gradient_Descent_fix_stepsize.x E inst inst_1 inst_2 f f' x0 self k -\n      @Gradient_Descent_fix_stepsize.a E inst inst_1 inst_2 f f' x0 self •\n        f' (@Gradient_Descent_fix_stepsize.x E inst inst_1 inst_2 f f' x0 self k)",
  "declName": "Gradient_Descent_fix_stepsize.update",
  "decl":
  "class Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\nclass Gradient_Descent_fix_stepsize (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a • f' (x k))\n  (hl : l > (0 : ℝ)) (step₁ : a > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {inst : NormedAddCommGroup E} {inst_1 : InnerProductSpace ℝ E} {inst_2 : CompleteSpace E} {f : E → ℝ}\n  {f' : E → E} {x0 : E} [self : @GradientDescent E inst inst_1 inst_2 f f' x0],\n  LipschitzWith (@GradientDescent.l E inst inst_1 inst_2 f f' x0 self) f'",
  "declName": "GradientDescent.smooth",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  [inst : NormedAddCommGroup E] →\n    [inst_1 : InnerProductSpace ℝ E] →\n      [inst_2 : CompleteSpace E] →\n        {f : E → ℝ} →\n          {f' : E → E} →\n            {x0 : E} →\n              (x : ℕ → E) →\n                (a : ℕ → ℝ) →\n                  (l : NNReal) →\n                    (∀ (x₁ : E), HasGradientAt f (f' x₁) x₁) →\n                      LipschitzWith l f' →\n                        (∀ (k : ℕ), x (k + 1) = x k - a k • f' (x k)) →\n                          l > 0 → (∀ (k : ℕ), a k > 0) → x 0 = x0 → @GradientDescent E inst inst_1 inst_2 f f' x0",
  "declName": "GradientDescent.mk",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {inst : NormedAddCommGroup E} {inst_1 : InnerProductSpace ℝ E} {inst_2 : CompleteSpace E} {f : E → ℝ}\n  {f' : E → E} {x0 : E} [self : @GradientDescent E inst inst_1 inst_2 f f' x0],\n  @GradientDescent.x E inst inst_1 inst_2 f f' x0 self 0 = x0",
  "declName": "GradientDescent.initial",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  [inst : NormedAddCommGroup E] →\n    [inst_1 : InnerProductSpace ℝ E] →\n      [inst_2 : CompleteSpace E] →\n        {f : E → ℝ} →\n          {f' : E → E} →\n            {x0 : E} →\n              {motive : @GradientDescent E inst inst_1 inst_2 f f' x0 → Sort u} →\n                (t : @GradientDescent E inst inst_1 inst_2 f f' x0) →\n                  ((x : ℕ → E) →\n                      (a : ℕ → ℝ) →\n                        (l : NNReal) →\n                          (diff : ∀ (x₁ : E), HasGradientAt f (f' x₁) x₁) →\n                            (smooth : LipschitzWith l f') →\n                              (update : ∀ (k : ℕ), x (k + 1) = x k - a k • f' (x k)) →\n                                (hl : l > 0) →\n                                  (step₁ : ∀ (k : ℕ), a k > 0) →\n                                    (initial : x 0 = x0) →\n                                      motive\n                                        (@GradientDescent.mk E inst inst_1 inst_2 f f' x0 x a l diff smooth update hl\n                                          step₁ initial)) →\n                    motive t",
  "declName": "GradientDescent.casesOn",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {inst : NormedAddCommGroup E} {inst_1 : InnerProductSpace ℝ E} {inst_2 : CompleteSpace E} {f : E → ℝ}\n  {f' : E → E} {x0 : E} [self : @GradientDescent E inst inst_1 inst_2 f f' x0] (k : ℕ),\n  @GradientDescent.x E inst inst_1 inst_2 f f' x0 self (k + 1) =\n    @GradientDescent.x E inst inst_1 inst_2 f f' x0 self k -\n      @GradientDescent.a E inst inst_1 inst_2 f f' x0 self k •\n        f' (@GradientDescent.x E inst inst_1 inst_2 f f' x0 self k)",
  "declName": "GradientDescent.update",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  {inst : NormedAddCommGroup E} →\n    {inst_1 : InnerProductSpace ℝ E} →\n      {inst_2 : CompleteSpace E} →\n        (f : E → ℝ) → (f' : E → E) → (x0 : E) → [self : @GradientDescent E inst inst_1 inst_2 f f' x0] → ℕ → E",
  "declName": "GradientDescent.x",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  [inst : NormedAddCommGroup E] →\n    [inst_1 : InnerProductSpace ℝ E] →\n      [inst_2 : CompleteSpace E] →\n        {f : E → ℝ} →\n          {f' : E → E} →\n            {x0 : E} →\n              {motive : @GradientDescent E inst inst_1 inst_2 f f' x0 → Sort u} →\n                (t : @GradientDescent E inst inst_1 inst_2 f f' x0) →\n                  ((x : ℕ → E) →\n                      (a : ℕ → ℝ) →\n                        (l : NNReal) →\n                          (diff : ∀ (x₁ : E), HasGradientAt f (f' x₁) x₁) →\n                            (smooth : LipschitzWith l f') →\n                              (update : ∀ (k : ℕ), x (k + 1) = x k - a k • f' (x k)) →\n                                (hl : l > 0) →\n                                  (step₁ : ∀ (k : ℕ), a k > 0) →\n                                    (initial : x 0 = x0) →\n                                      motive\n                                        (@GradientDescent.mk E inst inst_1 inst_2 f f' x0 x a l diff smooth update hl\n                                          step₁ initial)) →\n                    motive t",
  "declName": "GradientDescent.recOn",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  [inst : NormedAddCommGroup E] →\n    [inst_1 : InnerProductSpace ℝ E] → [inst : CompleteSpace E] → (E → ℝ) → (E → E) → E → Type u_1",
  "declName": "GradientDescent",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  {inst : NormedAddCommGroup E} →\n    {inst_1 : InnerProductSpace ℝ E} →\n      {inst_2 : CompleteSpace E} →\n        (f : E → ℝ) → (f' : E → E) → (x0 : E) → [self : @GradientDescent E inst inst_1 inst_2 f f' x0] → NNReal",
  "declName": "GradientDescent.l",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {inst : NormedAddCommGroup E} {inst_1 : InnerProductSpace ℝ E} {inst_2 : CompleteSpace E} {f : E → ℝ}\n  {f' : E → E} (x0 : E) [self : @GradientDescent E inst inst_1 inst_2 f f' x0] (x₁ : E), HasGradientAt f (f' x₁) x₁",
  "declName": "GradientDescent.diff",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {inst : NormedAddCommGroup E} {inst_1 : InnerProductSpace ℝ E} {inst_2 : CompleteSpace E} {f : E → ℝ}\n  {f' : E → E} {x0 : E} [self : @GradientDescent E inst inst_1 inst_2 f f' x0] (k : ℕ),\n  @GradientDescent.a E inst inst_1 inst_2 f f' x0 self k > 0",
  "declName": "GradientDescent.step₁",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {inst : NormedAddCommGroup E} {inst_1 : InnerProductSpace ℝ E} {inst_2 : CompleteSpace E} {f : E → ℝ}\n  {f' : E → E} {x0 : E} [self : @GradientDescent E inst inst_1 inst_2 f f' x0],\n  @GradientDescent.l E inst inst_1 inst_2 f f' x0 self > 0",
  "declName": "GradientDescent.hl",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  {inst : NormedAddCommGroup E} →\n    {inst_1 : InnerProductSpace ℝ E} →\n      {inst_2 : CompleteSpace E} →\n        {f : E → ℝ} →\n          {f' : E → E} →\n            {x0 : E} →\n              {P : Sort u} →\n                {v1 v2 : @GradientDescent E inst inst_1 inst_2 f f' x0} →\n                  v1 = v2 → @GradientDescent.noConfusionType E inst inst_1 inst_2 f f' x0 P v1 v2",
  "declName": "GradientDescent.noConfusion",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  {inst : NormedAddCommGroup E} →\n    {inst_1 : InnerProductSpace ℝ E} →\n      {inst_2 : CompleteSpace E} →\n        (f : E → ℝ) → (f' : E → E) → (x0 : E) → [self : @GradientDescent E inst inst_1 inst_2 f f' x0] → ℕ → ℝ",
  "declName": "GradientDescent.a",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "{E : Type u_1} →\n  [inst : NormedAddCommGroup E] →\n    [inst_1 : InnerProductSpace ℝ E] →\n      [inst_2 : CompleteSpace E] →\n        {f : E → ℝ} →\n          {f' : E → E} →\n            {x0 : E} →\n              {motive : @GradientDescent E inst inst_1 inst_2 f f' x0 → Sort u} →\n                ((x : ℕ → E) →\n                    (a : ℕ → ℝ) →\n                      (l : NNReal) →\n                        (diff : ∀ (x₁ : E), HasGradientAt f (f' x₁) x₁) →\n                          (smooth : LipschitzWith l f') →\n                            (update : ∀ (k : ℕ), x (k + 1) = x k - a k • f' (x k)) →\n                              (hl : l > 0) →\n                                (step₁ : ∀ (k : ℕ), a k > 0) →\n                                  (initial : x 0 = x0) →\n                                    motive\n                                      (@GradientDescent.mk E inst inst_1 inst_2 f f' x0 x a l diff smooth update hl\n                                        step₁ initial)) →\n                  (t : @GradientDescent E inst inst_1 inst_2 f f' x0) → motive t",
  "declName": "GradientDescent.rec",
  "decl":
  "class GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\nend descent_lemma\n\nnoncomputable section gradient_descent\n\nvariable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]\n\nclass GradientDescent (f : E → ℝ) (f' : E → E) (x0 : E) where\n  (x : ℕ → E) (a : ℕ → ℝ) (l : NNReal)\n  (diff : ∀ x₁, HasGradientAt f (f' x₁) x₁) (smooth : LipschitzWith l f')\n  (update : ∀ k : ℕ, x (k + 1) = x k - a k • f' (x k))\n  (hl : l > 0) (step₁ : ∀ k, a k > 0) (initial : x 0 = x0)\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {xm : E} {f : E → ℝ} {g : ℕ → E},\n  (∀ (k : ℕ), f (g (k + 1)) ≤ f (g k)) →\n    ∀ (n : ℕ), f (g (n + 1)) - f xm ≤ (∑ k ∈ Finset.range (n + 1), (f (g (k + 1)) - f xm)) / (↑n + 1)",
  "declName": "mono_sum_prop",
  "decl":
  "omit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop (mono : ∀ k: ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ ,  (f (g (n + 1)) -  f xm) ≤ (Finset.range (n + 1)).sum\n    (fun (k : ℕ) ↦ f (g (k + 1)) - f xm) / (n + 1) := by\n  intro n\n  induction' n with j _\n  · simp\n  · simp\n    calc f (g (j + 2)) ≤ (Finset.range (j.succ + 1)).sum\n          (fun (k : ℕ) ↦ f (g (k + 1))) / (j.succ + 1) := by\n            linarith [mono_sum_prop_primal' mono j]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            / (j + 2) - f xm * 1 + f xm := by\n          rw [Nat.succ_eq_add_one j]; simp\n          ring_nf; rw [add_assoc, one_add_one_eq_two]\n      _ = (Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (j + 2)\n            - f xm * ((j + 2) / (j + 2)) + f xm := by field_simp\n      _ = ((Finset.range (j.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1)))\n            - (j + 1 + 1) * f xm) / (j + 1+1)+ f xm := by\n          simp; rw [← one_add_one_eq_two, ← add_assoc, mul_div, mul_comm, ← sub_div]\n\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {f : E → ℝ} {g : ℕ → E},\n  (∀ (k : ℕ), f (g (k + 1)) ≤ f (g k)) →\n    ∀ (n : ℕ), (∑ k ∈ Finset.range (n.succ + 1), f (g (k + 1))) / (↑n.succ + 1) ≥ f (g (n + 2))",
  "declName": "mono_sum_prop_primal'",
  "decl":
  "omit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal' (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ, (Finset.range (n.succ + 1)).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (n.succ + 1)\n      ≥ f (g (n + 2)) := by\n  intro n\n  have h : (n + 1) * f (g (n.succ + 1)) / (↑n + 1 + 1)\n        ≤ (Finset.range n.succ).sum (fun (k : ℕ) ↦ f (g (k + 1))) / (↑n + 1 + 1) :=\n    div_le_div_of_nonneg_right (mono_sum_prop_primal mono (n.succ - 1)) (by linarith)\n  calc\n    _ = ((Finset.range (n.succ)).sum (fun (k : ℕ) ↦ f (g (k + 1)))) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by rw [Finset.sum_range_succ, add_div]\n    _ ≥ n.succ * f (g (n.succ + 1)) / (n.succ + 1)\n        + f (g (n.succ + 1)) / (n.succ + 1) := by simp; exact h\n    _ = f (g (n + 2)) := by field_simp; ring_nf\n\n-- the sumation property of the gradient method\n"},
 {"docString": null,
  "declType":
  "∀ {E : Type u_1} {f : E → ℝ} {g : ℕ → E},\n  (∀ (k : ℕ), f (g (k + 1)) ≤ f (g k)) → ∀ (n : ℕ), ∑ k ∈ Finset.range (n + 1), f (g (k + 1)) ≥ (↑n + 1) * f (g (n + 2))",
  "declName": "mono_sum_prop_primal",
  "decl":
  "omit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\n",
  "context":
  "/-!\n# GradientDescent\n\n## Main results\n\n  This file mainly concentrates on the Gradient Descent algorithm for\n  smooth convex optimization problems.\n\n  We prove the O(1 / k) rate for this algorithm.\n\n-/\n\n#check HasFDerivAtFilter.isLittleO\n\nsection descent_lemma\n\nvariable {E : Type*} [NormedAddCommGroup E]\n\nvariable {xm : E} {f : E → ℝ} {g : ℕ → E}\n\nopen Set Finset\n\n-- by monotonity of the sequence, we can get the bound for the sum of the sequence\nomit [NormedAddCommGroup E] in\nlemma mono_sum_prop_primal (mono : ∀ k : ℕ, f (g (k + 1)) ≤ f (g k)):\n    ∀ n : ℕ , (Finset.range (n + 1)).sum (fun k ↦ f (g (k + 1))) ≥\n      (n + (1 : ℝ)) * f (g (n + 2)) := by\n  intro n\n  induction' n with q IH1\n  · simp; apply mono 1\n  · specialize mono (q + 2)\n    calc (Finset.range (q.succ + 1)).sum (fun k ↦ f (g (k + 1)))\n            = (Finset.range (q + 1)).sum (fun k ↦ f (g (k + 1))) + f (g (q + 2)) := by\n              rw [Finset.sum_range_succ (fun k ↦ f (g (k + 1))) q.succ]\n          _ ≥ (q + (1 : ℝ)) * (f (g (q + 2))) + f (g (q + 2)) := by linarith\n          _ = (q + 2) * (f (g (q + 2))) := by ring_nf\n          _ ≥ (q + 2) * (f (g (q + 3))) := mul_le_mul_of_nonneg_left mono (by linarith)\n          _ = ((q.succ) + 1) * f (g (q.succ + 2)) := by simp; left; ring_nf\n\n-- for a certain iteration, we can get the bound by the sum of the sequence\n"}]
